{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":320111,"datasetId":134715,"databundleVersionId":333307,"isSourceIdPinned":false}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:30.416895Z","iopub.execute_input":"2025-07-07T11:31:30.417281Z","iopub.status.idle":"2025-07-07T11:31:30.639568Z","shell.execute_reply.started":"2025-07-07T11:31:30.417250Z","shell.execute_reply":"2025-07-07T11:31:30.637537Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:30.641416Z","iopub.execute_input":"2025-07-07T11:31:30.641873Z","iopub.status.idle":"2025-07-07T11:31:30.646421Z","shell.execute_reply.started":"2025-07-07T11:31:30.641847Z","shell.execute_reply":"2025-07-07T11:31:30.645342Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:30.647204Z","iopub.execute_input":"2025-07-07T11:31:30.647495Z","iopub.status.idle":"2025-07-07T11:31:31.431802Z","shell.execute_reply.started":"2025-07-07T11:31:30.647463Z","shell.execute_reply":"2025-07-07T11:31:31.430562Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(50000, 2)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n\n# Randomly sample 500 reviews\ndf= df.sample(n=500, random_state=42).reset_index(drop=True)\n\n# Check shape\nprint(\"Original size:\", df.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:31.434363Z","iopub.execute_input":"2025-07-07T11:31:31.434683Z","iopub.status.idle":"2025-07-07T11:31:32.191180Z","shell.execute_reply.started":"2025-07-07T11:31:31.434657Z","shell.execute_reply":"2025-07-07T11:31:32.190202Z"}},"outputs":[{"name":"stdout","text":"Original size: (500, 2)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import re # for regular expression\n#using regular expression for removing the html tags\ndef remove_html_tags(text):\n  pattern = re.compile('<.*?>')\n  return pattern.sub(r'',text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.192217Z","iopub.execute_input":"2025-07-07T11:31:32.192701Z","iopub.status.idle":"2025-07-07T11:31:32.198595Z","shell.execute_reply.started":"2025-07-07T11:31:32.192666Z","shell.execute_reply":"2025-07-07T11:31:32.197261Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def remove_url(text):\n  pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n  return pattern.sub(r'',text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.199769Z","iopub.execute_input":"2025-07-07T11:31:32.200043Z","iopub.status.idle":"2025-07-07T11:31:32.221289Z","shell.execute_reply.started":"2025-07-07T11:31:32.200019Z","shell.execute_reply":"2025-07-07T11:31:32.220070Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import string,time\nstring.punctuation\nexclude = string.punctuation\nprint(exclude)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.222363Z","iopub.execute_input":"2025-07-07T11:31:32.222678Z","iopub.status.idle":"2025-07-07T11:31:32.243818Z","shell.execute_reply.started":"2025-07-07T11:31:32.222638Z","shell.execute_reply":"2025-07-07T11:31:32.242677Z"}},"outputs":[{"name":"stdout","text":"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def remove_punc1(text):\n    return text.translate(str.maketrans('', '', exclude))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.244879Z","iopub.execute_input":"2025-07-07T11:31:32.245193Z","iopub.status.idle":"2025-07-07T11:31:32.263068Z","shell.execute_reply.started":"2025-07-07T11:31:32.245163Z","shell.execute_reply":"2025-07-07T11:31:32.261709Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"chat_words = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek You (also a chat program)\",\n    \"ILU\": \"I Love You\",\n    \"IMHO\": \"In My Honest/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laughing My A** Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A**\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"QPSA?\": \"Que Pasa?\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n     \"ROTFLMAO\": \"Rolling On The Floor Laughing My A** Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your sex and age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait...\",\n    \"7K\": \"Sick:-D Laugher\",\n    \"TFW\": \"That feeling when\",\n    \"MFW\": \"My face when\",\n    \"MRW\": \"My reaction when\",\n    \"IFYP\": \"I feel your pain\",\n    \"TNTL\": \"Trying not to laugh\",\n    \"JK\": \"Just kidding\",\n    \"IDC\": \"I don’t care\",\n    \"ILY\": \"I love you\",\n    \"IMU\": \"I miss you\",\n    \"ADIH\": \"Another day in hell\",\n    \"ZZZ\": \"Sleeping, bored, tired\",\n    \"WYWH\": \"Wish you were here\",\n    \"TIME\": \"Tears in my eyes\",\n    \"BAE\": \"Before anyone else\",\n    \"FIMH\": \"Forever in my heart\",\n    \"BSAAW\": \"Big smile and a wink\",\n    \"BWL\": \"Bursting with laughter\",\n    \"BFF\": \"Best friends forever\",\n    \"CSL\": \"Can’t stop laughing\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.264190Z","iopub.execute_input":"2025-07-07T11:31:32.265201Z","iopub.status.idle":"2025-07-07T11:31:32.346201Z","shell.execute_reply.started":"2025-07-07T11:31:32.265166Z","shell.execute_reply":"2025-07-07T11:31:32.345129Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def chat_conversation(text):\n  new_text = []\n  for w in text.split():\n    if w.upper() in chat_words:\n      new_text.append(chat_words[w.upper()])\n    else:\n      new_text.append(w)\n  return \" \".join(new_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.349791Z","iopub.execute_input":"2025-07-07T11:31:32.350084Z","iopub.status.idle":"2025-07-07T11:31:32.371442Z","shell.execute_reply.started":"2025-07-07T11:31:32.350062Z","shell.execute_reply":"2025-07-07T11:31:32.370368Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:31:32.372649Z","iopub.execute_input":"2025-07-07T11:31:32.372932Z","iopub.status.idle":"2025-07-07T11:32:06.031514Z","shell.execute_reply.started":"2025-07-07T11:31:32.372912Z","shell.execute_reply":"2025-07-07T11:32:06.030083Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def remove_stopwords(text):\n  new_text = []\n\n  for word in text.split():\n    if word in stopwords.words('english'):\n      new_text.append('')\n    else:\n      new_text.append(word)\n\n  x = new_text[:]\n  new_text.clear()\n  return \" \".join(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.032558Z","iopub.execute_input":"2025-07-07T11:32:06.033042Z","iopub.status.idle":"2025-07-07T11:32:06.041975Z","shell.execute_reply.started":"2025-07-07T11:32:06.033015Z","shell.execute_reply":"2025-07-07T11:32:06.040580Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df['review'] = df['review'].str.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.043160Z","iopub.execute_input":"2025-07-07T11:32:06.043467Z","iopub.status.idle":"2025-07-07T11:32:06.075042Z","shell.execute_reply.started":"2025-07-07T11:32:06.043445Z","shell.execute_reply":"2025-07-07T11:32:06.073722Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"df['review'] = df['review'].apply(remove_html_tags)\ndf['review'] = df['review'].apply(remove_url)\ndf['review'] = df['review'].apply(remove_punc1)\ndf['review'] = df['review'].apply(chat_conversation)\n# df['review'] = df['review'].apply(remove_stopwords)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.076677Z","iopub.execute_input":"2025-07-07T11:32:06.077021Z","iopub.status.idle":"2025-07-07T11:32:06.154845Z","shell.execute_reply.started":"2025-07-07T11:32:06.076999Z","shell.execute_reply":"2025-07-07T11:32:06.153596Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Problem 2\n\n# Find out the number of words in the entire corpus and also the total number of unique words(vocabulary) using just python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.155958Z","iopub.execute_input":"2025-07-07T11:32:06.156215Z","iopub.status.idle":"2025-07-07T11:32:06.160738Z","shell.execute_reply.started":"2025-07-07T11:32:06.156193Z","shell.execute_reply":"2025-07-07T11:32:06.159814Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"all_words=[]\nfor review in df['review']:\n  words = review.split()\n  all_words.extend(words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.162127Z","iopub.execute_input":"2025-07-07T11:32:06.162476Z","iopub.status.idle":"2025-07-07T11:32:06.194290Z","shell.execute_reply.started":"2025-07-07T11:32:06.162446Z","shell.execute_reply":"2025-07-07T11:32:06.193025Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Total number of words in the corpus\ntotal_words = len(all_words)\n\nunique_words = set(all_words)\nvocab_size = len(unique_words)\n\nprint(\"Total number of words in corpus:\", total_words)\nprint(\"Vocabulary size (unique words):\", vocab_size)\n# print(\"Vocabulary:\", unique_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.195248Z","iopub.execute_input":"2025-07-07T11:32:06.195544Z","iopub.status.idle":"2025-07-07T11:32:06.225870Z","shell.execute_reply.started":"2025-07-07T11:32:06.195524Z","shell.execute_reply":"2025-07-07T11:32:06.224725Z"}},"outputs":[{"name":"stdout","text":"Total number of words in corpus: 112621\nVocabulary size (unique words): 14008\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Problem 3\n\n# Apply One Hot Encoding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.226921Z","iopub.execute_input":"2025-07-07T11:32:06.227189Z","iopub.status.idle":"2025-07-07T11:32:06.244706Z","shell.execute_reply.started":"2025-07-07T11:32:06.227170Z","shell.execute_reply":"2025-07-07T11:32:06.243569Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Assuming you already have these:\n# reviews = [ ... ]         # List of review strings\n# all_words = [ ... ]       # List of all words (with duplicates)\n# unique_words = { ... }    # Set of unique words\n\n# Step 1: Create vocabulary index mapping\nvocab = sorted(unique_words)  # optional: sorted for consistency\nword2idx = {word: idx for idx, word in enumerate(vocab)}\n\n# Step 2: Convert each review into a one-hot vector\nimport string\n\nohe_vectors = []\n\nfor review in df['review']:\n    # Clean and tokenize review\n    review = review.translate(str.maketrans('', '', string.punctuation)).lower()\n    tokens = review.split()\n    \n    # Initialize zero vector of vocab length\n    vector = [0] * len(word2idx)\n    \n    # Set 1 for each word present in the review\n    for word in set(tokens):  # set() removes duplicates\n        if word in word2idx:\n            vector[word2idx[word]] = 1\n\n    ohe_vectors.append(vector)\n\n# Display (optional)\nprint(\"Vocabulary Size:\", len(word2idx))\n# print(\"First 5 Encoded Reviews:\")\n# for vec in ohe_vectors[:5]:\n#     print(vec)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:49.826842Z","iopub.execute_input":"2025-07-07T11:32:49.827167Z","iopub.status.idle":"2025-07-07T11:32:49.945808Z","shell.execute_reply.started":"2025-07-07T11:32:49.827144Z","shell.execute_reply":"2025-07-07T11:32:49.944746Z"}},"outputs":[{"name":"stdout","text":"Vocabulary Size: 14008\nFirst 5 Encoded Reviews:\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# Problem 4\n\n# Apply bag words and find the vocabulary also find the times each word has occured","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.379875Z","iopub.execute_input":"2025-07-07T11:32:06.380179Z","iopub.status.idle":"2025-07-07T11:32:06.384490Z","shell.execute_reply.started":"2025-07-07T11:32:06.380156Z","shell.execute_reply":"2025-07-07T11:32:06.383495Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import pandas as pd\nimport string\nfrom collections import Counter\n\n# Step 1: Create vocabulary and index mapping\nvocab = sorted(unique_words)  # optional: consistent column order\nword2idx = {word: idx for idx, word in enumerate(vocab)}\n\n# Step 2: Initialize BoW matrix\nbow_matrix = []\n\nfor review in df['review']:\n    # Clean and tokenize\n    clean = review.translate(str.maketrans('', '', string.punctuation)).lower()\n    tokens = clean.split()\n    \n    # Count frequencies in this review\n    freq = Counter(tokens)\n    \n    # Create row vector for this review\n    row = [freq.get(word, 0) for word in vocab]\n    bow_matrix.append(row)\n\n# Step 3: Convert to DataFrame\ndf_bow = pd.DataFrame(bow_matrix, columns=vocab)\n\n# Now df_bow[i][j] = count of word j in review i\nprint(\"Shape:\", df_bow.shape)\nprint(df_bow.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:39:03.834688Z","iopub.execute_input":"2025-07-07T11:39:03.835017Z","iopub.status.idle":"2025-07-07T11:39:07.145465Z","shell.execute_reply.started":"2025-07-07T11:39:03.834996Z","shell.execute_reply":"2025-07-07T11:39:07.144264Z"}},"outputs":[{"name":"stdout","text":"Shape: (500, 14008)\n   0  010  02  04  044  0that  1  10  100  1000  ...  zomedy  zone  zorro  \\\n0  0    0   0   0    0      0  0   0    0     0  ...       0     0      0   \n1  0    0   0   0    0      0  0   0    0     0  ...       0     0      0   \n2  0    0   0   0    0      0  0   0    0     0  ...       0     0      0   \n3  0    0   0   0    0      0  0   0    0     0  ...       0     0      0   \n4  0    0   0   0    0      0  0   0    0     0  ...       0     0      0   \n\n   zuckermanfill  emthe      £300  ‘dr  “at  \n0              0       0  0  0     0    0    0  \n1              0       0  0  0     0    0    0  \n2              0       0  0  0     0    0    0  \n3              0       0  0  0     0    0    0  \n4              0       0  0  0     0    0    0  \n\n[5 rows x 14008 columns]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"# Problem 5\n\n# Apply bag of bi-gram and bag of tri-gram and write down your observation about the dimensionality of the vocabulary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.385753Z","iopub.execute_input":"2025-07-07T11:32:06.386399Z","iopub.status.idle":"2025-07-07T11:32:06.404943Z","shell.execute_reply.started":"2025-07-07T11:32:06.386362Z","shell.execute_reply":"2025-07-07T11:32:06.403573Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import pandas as pd\nimport string\nfrom collections import Counter\nfrom itertools import islice\n\n# Helper: generate n-grams from a list of tokens\ndef generate_ngrams(tokens, n):\n    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n\n# Step 1: Build n-gram vocabulary from all reviews\ndef build_ngram_vocab(reviews, n):\n    vocab_set = set()\n    for review in reviews:\n        clean = review.translate(str.maketrans('', '', string.punctuation)).lower()\n        tokens = clean.split()\n        ngrams = generate_ngrams(tokens, n)\n        vocab_set.update(ngrams)\n    return sorted(vocab_set)  # consistent order\n\n# Step 2: Build Bag of N-Grams Matrix\ndef build_bow_matrix(reviews, vocab, n):\n    vocab_index = {ngram: idx for idx, ngram in enumerate(vocab)}\n    matrix = []\n\n    for review in reviews:\n        clean = review.translate(str.maketrans('', '', string.punctuation)).lower()\n        tokens = clean.split()\n        ngrams = generate_ngrams(tokens, n)\n        freq = Counter(ngrams)\n        row = [freq.get(ngram, 0) for ngram in vocab]\n        matrix.append(row)\n\n    return pd.DataFrame(matrix, columns=vocab)\n\n# 👇 Input: `reviews` must already be defined\n# For Bi-gram\nbigram_vocab = build_ngram_vocab(df['review'], 2)\ndf_bigram = build_bow_matrix(df['review'], bigram_vocab, 2)\n\n# For Tri-gram\ntrigram_vocab = build_ngram_vocab(df['review'], 3)\ndf_trigram = build_bow_matrix(df['review'], trigram_vocab, 3)\n\n# Optional: Save to CSV\n# df_bigram.to_csv(\"bigram_bow.csv\", index=False)\n# df_trigram.to_csv(\"trigram_bow.csv\", index=False)\n\n# Show shape & sample\nprint(\"Bigram shape:\", df_bigram.shape)\nprint(df_bigram.head(2))\n\nprint(\"\\nTrigram shape:\", df_trigram.shape)\nprint(df_trigram.head(2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:45:28.354797Z","iopub.execute_input":"2025-07-07T11:45:28.355118Z","iopub.status.idle":"2025-07-07T11:46:10.476986Z","shell.execute_reply.started":"2025-07-07T11:45:28.355098Z","shell.execute_reply":"2025-07-07T11:46:10.475854Z"}},"outputs":[{"name":"stdout","text":"Bigram shape: (500, 65917)\n   0 rating  02 i  04 supporters  044 the  0that said  1 2  1 5  1 and  1 at  \\\n0         0     0              0        0           0    0    0      0     0   \n1         0     0              0        0           0    0    0      0     0   \n\n   1 comedy  ...   or   perhaps   the   until   wants   with   it  \\\n0         0  ...     0          0      0        0        0       0     0   \n1         0  ...     0          0      0        0        0       0     0   \n\n   £300 it  ‘dr tarrs  “at least  \n0        0          0          0  \n1        0          0          0  \n\n[2 rows x 65917 columns]\n\nTrigram shape: (500, 100228)\n   0 rating i  02 i was  04 supporters are  044 the big  0that said she  \\\n0           0         0                  0            0               0   \n1           0         0                  0            0               0   \n\n   1 2 dead  1 5 sucked  1 and jumps  1 at the  1 comedy is  ...   or was  \\\n0         0           0            0         0            0  ...         0   \n1         0           0            0         0            0  ...         0   \n\n    perhaps inadvertently   the weakest   until it   wants youalthough  \\\n0                        0              0           0                    0   \n1                        0              0           0                    0   \n\n    with only   it blows  £300 it looked  ‘dr tarrs torture  “at least i  \n0            0           0               0                  0            0  \n1            0           0               0                  0            0  \n\n[2 rows x 100228 columns]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Problem 6\n\n# Apply tf-idf and find out the idf scores of words, also find out the vocabulary.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:32:06.405949Z","iopub.execute_input":"2025-07-07T11:32:06.406276Z","iopub.status.idle":"2025-07-07T11:32:06.423846Z","shell.execute_reply.started":"2025-07-07T11:32:06.406248Z","shell.execute_reply":"2025-07-07T11:32:06.422756Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import math\nimport pandas as pd\nimport string\nfrom collections import Counter\n\n# Step 1: Clean and tokenize reviews\ntokenized_reviews = []\n\nfor review in df['review']:\n    clean = review.translate(str.maketrans('', '', string.punctuation)).lower()\n    tokens = clean.split()\n    tokenized_reviews.append(tokens)\n\n# Step 2: Build IDF scores\nN = len(tokenized_reviews)  # Total number of documents\nword_doc_count = {}\n\nfor tokens in tokenized_reviews:\n    unique_in_review = set(tokens)\n    for word in unique_in_review:\n        word_doc_count[word] = word_doc_count.get(word, 0) + 1\n\nidf_scores = {}\nfor word in word_doc_count:\n    df = word_doc_count[word]\n    idf = math.log(N / (1 + df))  # smoothing\n    idf_scores[word] = idf\n\n# Step 3: (Optional) Build TF-IDF Matrix\nvocab = sorted(idf_scores.keys())\ntfidf_matrix = []\n\nfor tokens in tokenized_reviews:\n    word_freq = Counter(tokens)\n    total_words = len(tokens)\n    row = []\n    for word in vocab:\n        tf = word_freq[word] / total_words if word in word_freq else 0\n        tfidf = tf * idf_scores[word] if tf > 0 else 0\n        row.append(tfidf)\n    tfidf_matrix.append(row)\n\n# Create DataFrame\ndf_tfidf = pd.DataFrame(tfidf_matrix, columns=vocab)\n\n# Show TF-IDF matrix shape and sample\nprint(\"TF-IDF matrix shape:\", df_tfidf.shape)\nprint(df_tfidf.head(2))\n\n# Step 4: Show IDF scores\nidf_df = pd.DataFrame(idf_scores.items(), columns=[\"word\", \"idf\"]).sort_values(by=\"idf\", ascending=False)\nprint(\"\\nTop 10 IDF scores (rarest words):\")\nprint(idf_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T11:50:40.326351Z","iopub.execute_input":"2025-07-07T11:50:40.326721Z","iopub.status.idle":"2025-07-07T11:50:45.856635Z","shell.execute_reply.started":"2025-07-07T11:50:40.326688Z","shell.execute_reply":"2025-07-07T11:50:45.854997Z"}},"outputs":[{"name":"stdout","text":"TF-IDF matrix shape: (500, 13989)\n     0  010   02   04  044  0that    1   10  100  1000  ...  zomedy  zone  \\\n0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0   0.0  ...     0.0   0.0   \n1  0.0  0.0  0.0  0.0  0.0    0.0  0.0  0.0  0.0   0.0  ...     0.0   0.0   \n\n   zorro  zuckermanfill  emthe          £300  ‘dr  “at  \n0    0.0            0.0     0.0  0.0  0.0   0.0  0.0  0.0  \n1    0.0            0.0     0.0  0.0  0.0   0.0  0.0  0.0  \n\n[2 rows x 13989 columns]\n\nTop 10 IDF scores (rarest words):\n              word       idf\n0       undertaker  5.521461\n9322      kinnears  5.521461\n9294        angora  5.521461\n9295          stan  5.521461\n9296        hardys  5.521461\n9297       bestthe  5.521461\n9299        dayits  5.521461\n9301       present  5.521461\n9302  thunderbirds  5.521461\n9303        corbet  5.521461\n","output_type":"stream"}],"execution_count":40}]}